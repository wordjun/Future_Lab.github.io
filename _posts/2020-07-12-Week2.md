---
published: true
---
[미래연구소](http://futurelab.creatorlink.net/)

> 딥러닝은 하나의 거대한 합성함수이다

이미지 처리방식-> 픽셀값(숫자)들을 1차원 배열로 변환(flatten)

feature vector의 차원수를 나타내는 N(Nx) = feature의 개수 또한 나타냄

Notation: 
x = 데이터 1개의 input = feature vector
y = data 1개의 label = 0혹은 1의 값(binary classification)
(x, y) = training data (여러개가 있다, m개가 있다)
xi(모든 x는 이미지를 flatten한것)
x1, x2, ...., xm: 이미지 m개
vectorization: X = (Nx, m) 행렬화

**Logistic Regression**
로지스틱 회귀는 이진분류를 위한 알고리즘이다.
예) 고양이 사진이 주어졌을 때, 이를 고양이(1) 혹은 고양이가 아니다(0)라고 판단하고 출력하고 싶은 경우,
y로 출력 label을 표시한다. 

사진이 컴퓨터에 저장되는 법: 해당 사진의 빨강, 초록, 파랑 채널들과 일치하는 3개의 행렬을 따로 저장한다. 그러므로 입력된 사진이 64x64 픽셀인 경우 64x64행렬들이 사진의 빨강, 초록, 파랑색 픽셀 강도 값을 나타낸다. 이 픽셀 강도값들을 특성 벡터로 바꾸려면 모든 픽셀값을 입력될 특성벡터 x의 한 열로 나열할 것이다.

픽셀값을 하나의 베거로 펼치기 위해 주어진 사진에 대한 특성 벡터 x를 다음과 같이 정의한다: 3개의 채널(RGB)의 값들이 하나의 열에 수록되도록 매우 긴 특성 벡터가 되도록 해당 사진의 모든 RGB픽셀 강도값들을 나열한다. 사진이 64x64인 경우 전체 차원은 64x64x3=12288이 된다. 이 차원을 nx로 나타내고, n이라고 표기할 수 있다.

이진 분류의 목표는 입력된 사진을 나타내는 특성벡터 x를 갖고 그에 대한 label y가 1아니면 0, 즉 고양이인지 아닌지를 예측할 수 있는 분류기를 학습하는 것이다.

단 하나의 훈련 샘플을 한 쌍(x, y)으로 표시할 때, x는 nx차원 상의 특성 벡터이고 레이블 y는 0 혹은 1이다. 첫번째 훈련 샘플의 입력과 출력인 (x^1, y^1), 두번째 훈련샘플인 (x^2, y^2)...(x^m, y^m)까지 모두 합한 것이 training set이 된다. 이 샘플의 개수를 m이라 하고, m_train이라 표기한다. 테스트 셋에 대해선 m_test라 표기한다. 행렬 X의 첫 열은 x^1로, ... x^m까지 놓고, X는 m개의 열과 nx개의 행들로 이루어진다.

따라서 X는 nx * m행렬이고, 파이썬으로 구현 시 X.shape로 행렬의 차원을 알 수 있다. 이 경우 (nx, m)이 출력된다. 
출력될 label Y는? y의 값들을 열로 놓는 것이 편하다.(신경망 구현) Y는 y^1~y^m으로 이루어진 1xm행렬로 정의한다. 따라서 Y.shape=(1, m)이 된다. 

신경망 구현 시 x나 y 또는 나중에 볼 여러 훈련샘플에 관한 데이터를 각각의 열로 놓는것이 유용하다.


Computation
함수 J(a, b, c)=2(a+bc)를 계산한다고 한다:
먼저 bc를 셰간해야하는데, u=bc, v=a+u라 하자.
그러면 J=3v가 된다.
예로, a=5, b = 3, c=2일때, u=6이고, v=11이므로 J는 33이 된다. (5 + 3*2)*3 = 33
계산 그래프는 J같은 특정 풀력값 변수를 최적화할 때 유용함. 왼쪽에서 오른쪽의 패스로 J값을 알 수 있다.

**Logistic Regression(선형 회귀)의 과정:**
gradient descent에서 cost J가 최소가 되는 지점을 찾는다.
weight값을 기울기값을 통해 수시로 업데이트하며 내려감

1. 시작점 지정(초기값을 정하는 것)
-W와 b를 0과 0으로 초기화(zero initialization)
-혹은 W와 b가 주어진다

2. 이를 기반으로 y hat값을 구하여 예측값을 계산 (Forward propagation)
Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 ‘forward’ propagation이라 한다.) 이는 신경망의 출력값을 계산한다.

Sigmoid함수
-y hat이 0.5이상인 경우, 1로 수렴 -> 1이라고 추정
-y hat이 0.5 미만인 경우 0으로 수렴 -> 0이라고 추정

좋은 예측모델은 실제 값인 y와 추정 값인 y hat의 차이가 적은 모델을 뜻한다.

3. cost function 계산
![]({{site.baseurl}}/https://wikimedia.org/api/rest_v1/media/math/render/svg/67b9ac7353c6a2710e35180238efe54faf4d9c15)
MSE(mean squared error)-오차의 제곱에 평균을 취한 것. 작을 수록 실제 값과의
오차가 적은 것이므로, 추측한 값의 정확성이 높음을 의미함

mse를 binary classification에 쓰지 않는 이유: local minimum이 많이 생기게 된다.
=> Binary cross-entropy function을 사용해 y hat과 y의 차이를 구한다 (yloga + (1-y)log(1-a)를 계싼하여 J에 누적)

4. cost function에 대한 gradient를 구해야 한다 (Backward propagation)
gradient값들을 구하는 법: 미분을 통해 구한다
Chain rule을 통해 기울기 계산. (경사나 도함수를 계산)

Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 이전 layer의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 ‘back’ propagation이라 한다.)

5. Gradient descent for logistic regression
back propagation하여 얻은 기울기 값을 weight에 곱하여 해당 값을 기존 weight에서 빼준다



**Vectorization**
벡터화는 for loop을 통해 하나의 값씩 계산하는 것보다 벡터화하여
한번에 행렬연산으로 처리하는것이 훨씬 더 빠르고 간결하기 때문에 쓴다.
총 m개의 훈련 데이터(training dataset)를 처리할 때, for문으로 m개의 데이터를 하나씩 보는 것보다