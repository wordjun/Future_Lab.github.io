---
published: false
---
> 딥러닝은 하나의 거대한 합성함수이다

이미지 처리방식-> 픽셀값(숫자)들을 1차원 배열로 변환(flatten)

feature vector의 차원수를 나타내는 N(Nx) = feature의 개수 또한 나타냄

Notation: 
x = 데이터 1개의 input = feature vector
y = data 1개의 label = 0혹은 1의 값(binary classification)
(x, y) = training data (여러개가 있다, m개가 있다)
xi(모든 x는 이미지를 flatten한것)
x1, x2, ...., xm: 이미지 m개
vectorization: X = (Nx, m) 행렬화


**Logistic Regression(선형 회귀)의 과정:**
gradient descent에서 cost J가 최소가 되는 지점을 찾는다.
weight값을 기울기값을 통해 수시로 업데이트하며 내려감

1. 시작점 지정(초기값을 정하는 것)
-W와 b를 0과 0으로 초기화(zero initialization)
-혹은 W와 b가 주어진다

2.이를 기반으로 y hat값을 구하여 예측값을 계산 (Forward propagation)
Forward propagation: input training data로부터 output을 계산하고, 각 ouput neuron에서의 error를 계산한다. (input -> hidden -> output 으로 정보가 흘러가므로 ‘forward’ propagation이라 한다.)

Sigmoid함수
-y hat이 0.5이상인 경우, 1로 수렴 -> 1이라고 추정
-y hat이 0.5 미만인 경우 0으로 수렴 -> 0이라고 추정

좋은 예측모델은 실제 값인 y와 추정 값인 y hat의 차이가 적은 모델을 뜻한다.

3. cost function 계산
![]({{site.baseurl}}/https://wikimedia.org/api/rest_v1/media/math/render/svg/67b9ac7353c6a2710e35180238efe54faf4d9c15)
MSE(mean squared error)-오차의 제곱에 평균을 취한 것. 작을 수록 실제 값과의
오차가 적은 것이므로, 추측한 값의 정확성이 높음을 의미함

mse를 binary classification에 쓰지 않는 이유: local minimum이 많이 생기게 된다.
=> Binary cross-entropy function을 사용해 y hat과 y의 차이를 구한다 (yloga + (1-y)log(1-a)를 계싼하여 J에 누적)

4. cost function에 대한 gradient를 구해야 한다 (Backward propagation)
gradient값들을 구하는 법: 미분을 통해 구한다
Chain rule을 통해 기울기 계산

Back propagation: output neuron에서 계산된 error를 각 edge들의 weight를 사용해 바로 이전 layer의 neuron들이 얼마나 error에 영향을 미쳤는지 계산한다. (output -> hidden 으로 정보가 흘러가므로 ‘back’ propagation이라 한다.)

5. Gradient descent for logistic regression
back propagation하여 얻은 기울기 값을 weight에 곱하여 해당 값을 기존 weight에서 빼준다



**Vectorization**
